diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index a4d5f250b463..5372a9f5c0fc 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -2879,6 +2879,15 @@ config X86_SYSFB
 
 endmenu
 
+menu "Athena scheduler"
+
+config SCHED_ATHENA
+	bool "Enable support for Athena scheduler"
+	depends on NUMA
+	---help---
+		Include support for page-table aware cross-numa scheduling.
+		Please note that enabling this will disable HW counters for perf tool.
+endmenu
 
 menu "Executable file formats / Emulations"
 
@@ -2925,6 +2934,7 @@ config COMPAT
 	def_bool y
 	depends on IA32_EMULATION || X86_X32
 
+
 if COMPAT
 config COMPAT_FOR_U64_ALIGNMENT
 	def_bool y
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 45b2b1c93d04..2b8a764c7d25 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1762,6 +1762,18 @@ ssize_t x86_event_sysfs_show(char *page, u64 config, u64 event)
 static struct attribute_group x86_pmu_attr_group;
 static struct attribute_group x86_pmu_caps_group;
 
+#ifdef CONFIG_SCHED_ATHENA
+/**
+ * @brief  This may cause problem for perf to profile applications.
+ * @todo Athena: fix this.
+ * @return int 
+ */
+static int __init init_hw_perf_events(void)
+{
+        printk(KERN_DEBUG "[Athena] is enabled: only software events are available in perf.\n");
+        return 0;
+}
+#else
 static int __init init_hw_perf_events(void)
 {
 	struct x86_pmu_quirk *quirk;
@@ -1883,6 +1895,7 @@ static int __init init_hw_perf_events(void)
 	cpuhp_remove_state(CPUHP_PERF_X86_PREPARE);
 	return err;
 }
+#endif
 early_initcall(init_hw_perf_events);
 
 static inline void x86_pmu_read(struct perf_event *event)
diff --git a/include/linux/athena.h b/include/linux/athena.h
new file mode 100644
index 000000000000..0c4a3ddcf44e
--- /dev/null
+++ b/include/linux/athena.h
@@ -0,0 +1,25 @@
+#ifndef ATHENA_H
+#define ATHENA_H
+#include <linux/types.h>
+#include <linux/sched.h>
+
+struct athena_pmc_callbacks {
+	int 	(*pmcs_alloc_per_thread_data)(unsigned long,struct task_struct*);
+	void	(*pmcs_save_callback)(void*, int);
+	void	(*pmcs_restore_callback)(void*, int);
+	void 	(*pmcs_tbs_tick)(void*, int);
+	void	(*pmcs_exec_thread)(struct task_struct*);
+	void	(*pmcs_free_per_thread_data)(struct task_struct*);
+	void	(*pmcs_exit_thread)(struct task_struct*);
+	int		(*get_pmc_val)(struct task_struct* task, uint64_t pmc_event, uint64_t* value);
+};
+
+int get_pmc_val(struct task_struct* task, uint64_t pmc_event, uint64_t* value);
+int pmcs_alloc_per_thread_data(unsigned long clone_flags, struct task_struct *p);
+void pmcs_save_callback(void* prof, int cpu);
+void pmcs_restore_callback(void* prof, int cpu);
+void pmcs_tbs_tick(void* prof, int cpu);
+void pmcs_exec_thread(struct task_struct* tsk);
+void pmcs_free_per_thread_data(struct task_struct* tsk);
+void pmcs_exit_thread(struct task_struct* tsk);
+#endif
\ No newline at end of file
diff --git a/include/linux/numa.h b/include/linux/numa.h
index 110b0e5d0fb0..33c3eca23dcb 100644
--- a/include/linux/numa.h
+++ b/include/linux/numa.h
@@ -13,4 +13,17 @@
 
 #define	NUMA_NO_NODE	(-1)
 
+#ifdef CONFIG_NUMA
+#ifdef CONFIG_SCHED_ATHENA
+
+/**
+ * TODO: This should be dynamically get number of sockets
+ * Currently this value only works for 2 socket machine.
+ * For more socket machines, this should be increased.
+ */
+#define NR_NODES 2
+
+#endif /* CONFIG_SCHED_ATHENA */
+#endif
+
 #endif /* _LINUX_NUMA_H */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index ca3f3eae8980..181ffa8ecadc 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1168,6 +1168,10 @@ struct task_struct {
 	void				*security;
 #endif
 
+#ifdef CONFIG_SCHED_ATHENA 
+       void *pmc;                      /* Per-thread PMC-specific data. Would be allocated by the kernel module */
+       unsigned char prof_enabled;     /* This field must be one for the profiler to be active in the current task */
+#endif
 	/*
 	 * New fields for task_struct should be added above here, so that
 	 * they are included in the randomized portion of task_struct.
@@ -1764,4 +1768,57 @@ extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 #define TASK_SIZE_OF(tsk)	TASK_SIZE
 #endif
 
+#ifdef CONFIG_SCHED_ATHENA
+
+#define PMC_ESEL_UMASK_SHIFT 8
+#define PMC_ESEL_CMASK_SHIFT 24
+#define PMC_ESEL_ENTRY(event, umask, cmask)       \
+    (((event)&0xFFUL) |                           \
+     (((umask)&0xFFUL) << PMC_ESEL_UMASK_SHIFT) | \
+     (((cmask)&0xFFUL) << PMC_ESEL_CMASK_SHIFT))
+
+#define PMC_ESEL_USR (1ULL << 16)    /* User Mode */
+#define PMC_ESEL_OS (1ULL << 17)     /* Kernel Mode */
+#define PMC_ESEL_EDGE (1ULL << 18)   /* Edge detect */
+#define PMC_ESEL_PC (1ULL << 19)     /* Pin control */
+#define PMC_ESEL_INT (1ULL << 20)    /* APIC interrupt enable */
+#define PMC_ESEL_ANY (1ULL << 21)    /* Any thread */
+#define PMC_ESEL_ENABLE (1ULL << 22) /* Enable counters */
+#define PMC_ESEL_INV (1ULL << 23)    /* Invert counter mask */
+
+/* architectural performance counters (works on all Intel CPUs) */
+#define PMC_ARCH_CORE_CYCLES PMC_ESEL_ENTRY(0x3C, 0x00, 0)
+#define PMC_ARCH_INSTR_RETIRED PMC_ESEL_ENTRY(0xC0, 0x00, 0)
+#define PMC_ARCH_REF_CYCLES PMC_ESEL_ENTRY(0x3C, 0x01, 0)
+#define PMC_ARCH_LLC_REF PMC_ESEL_ENTRY(0x2E, 0x4F, 0)
+#define PMC_ARCH_LLC_MISSES PMC_ESEL_ENTRY(0x2E, 0x41, 0)
+#define PMC_ARCH_BRANCHES PMC_ESEL_ENTRY(0xC4, 0x00, 0)
+#define PMC_ARCH_BRANCH_MISSES PMC_ESEL_ENTRY(0xC5, 0x00, 0)
+
+/* this performance counter measures LLC misses as a proxy for mem bandwidth */
+#define PMC_LLC_MISSES (PMC_ARCH_LLC_MISSES | PMC_ESEL_USR | PMC_ESEL_OS | \
+                        PMC_ESEL_ENABLE)
+#define PMC_LLC_MISSES_ANY (PMC_ARCH_LLC_MISSES | PMC_ESEL_USR | PMC_ESEL_OS | \
+                            PMC_ESEL_ANY | PMC_ESEL_ENABLE)
+
+#define IA32_PERF_GLOBAL_CTRL_ENABLE 0x70000000f
+#define IA32_PERF_FIXED_CTRL_ENABLE 0x333
+
+#define PMC_UPDATE_PERIOD_MS 3000 /* The time interval to update the PMU counters */
+
+typedef struct {
+	u64 val;
+	u64 rate;
+	uint64_t tsc;  /* cycles. not used yet */
+	bool enable; /* enable or disable counter */
+} pmc_t;
+
+struct pmc_sample_struct {
+	unsigned long next_sample;  /* next jiffies to update the global sampling */
+	pmc_t llc_miss;
+	pmc_t dtlb_miss; /* not used yet */
+};
+
+#endif /* CONFIG_SCHED_ATHENA */
+
 #endif
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 715c97c2d1f6..e5b5169bf368 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -7,6 +7,10 @@
 
 struct ctl_table;
 
+#ifdef CONFIG_SCHED_ATHENA
+extern unsigned int sysctl_athena_enable; /* 1 to enable */
+#endif
+
 #ifdef CONFIG_DETECT_HUNG_TASK
 extern int	     sysctl_hung_task_check_count;
 extern unsigned int  sysctl_hung_task_panic;
@@ -98,4 +102,8 @@ int sysctl_numa_pgtable_replication_misc_ctl(struct ctl_table *table, int write,
 			void __user *buffer, size_t *lenp, loff_t *ppos);
 int sysctl_numa_migrate_pid_pgtable_ctl(struct ctl_table *table, int write,
 			void __user *buffer, size_t *lenp, loff_t *ppos);
+
+// athena sysctl handler
+int sched_athena_enable_handler(struct ctl_table *table, int write,
+			       void __user *buffer, size_t *lenp, loff_t *ppos);
 #endif /* _LINUX_SCHED_SYSCTL_H */
diff --git a/kernel/Makefile b/kernel/Makefile
index f85ae5dfa474..45cb7c11c874 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -42,6 +42,7 @@ obj-y += irq/
 obj-y += rcu/
 obj-y += livepatch/
 
+obj-$(CONFIG_SCHED_ATHENA) += athena.o
 obj-$(CONFIG_CHECKPOINT_RESTORE) += kcmp.o
 obj-$(CONFIG_FREEZER) += freezer.o
 obj-$(CONFIG_PROFILING) += profile.o
diff --git a/kernel/athena.c b/kernel/athena.c
new file mode 100644
index 000000000000..9ff5cf14c6bc
--- /dev/null
+++ b/kernel/athena.c
@@ -0,0 +1,162 @@
+#include <linux/athena.h>
+#include <linux/list.h>
+#include <linux/err.h>
+#include <linux/slab.h>
+#include <linux/rwlock.h>
+#include <linux/rcupdate.h>
+
+extern unsigned int sysctl_athena_enable;
+DEFINE_RWLOCK(pmc_ops_lock);
+
+struct athena_pmc_callbacks pmc_callbacks;
+
+EXPORT_SYMBOL(pmc_callbacks);
+
+
+/* Invoked when forking a process/thread */
+int pmcs_alloc_per_thread_data(unsigned long clone_flags, struct task_struct *p)
+{
+	int ret = 0;
+	unsigned long flags;
+
+	if(!sysctl_athena_enable)
+		return 0;
+
+	read_lock_irqsave(&pmc_ops_lock, flags);
+
+	if (!pmc_callbacks.pmcs_alloc_per_thread_data) {
+		read_unlock_irqrestore(&pmc_ops_lock, flags);
+		return 0;
+	}
+
+	read_unlock_irqrestore(&pmc_ops_lock, flags);
+
+	/* Invoke the allocation operation (may block) */
+	ret = pmc_callbacks.pmcs_alloc_per_thread_data(clone_flags, p);
+
+	return ret;
+}
+
+/* Invoked when a context switch out takes place */
+void pmcs_save_callback(void *prof, int cpu)
+{
+	if(!sysctl_athena_enable)
+		return;
+
+	if (!pmc_callbacks.pmcs_save_callback)
+		return;
+
+	rcu_read_lock();
+
+	pmc_callbacks.pmcs_save_callback(prof, cpu);
+
+	rcu_read_unlock();
+}
+
+/* Invoked when a context switch in takes place */
+void pmcs_restore_callback(void *prof, int cpu)
+{
+	if(!sysctl_athena_enable)
+		return;
+
+	if (!pmc_callbacks.pmcs_restore_callback)
+		return;
+
+	rcu_read_lock();
+
+	pmc_callbacks.pmcs_restore_callback(prof, cpu);
+
+	rcu_read_unlock();
+}
+
+/* Invoked from scheduler_tick() */
+void pmcs_tbs_tick(void *prof, int cpu)
+{
+	if(!sysctl_athena_enable)
+		return;
+
+	if (!pmc_callbacks.pmcs_tbs_tick)
+		return;
+
+	rcu_read_lock();
+
+	pmc_callbacks.pmcs_tbs_tick(prof, cpu);
+
+	rcu_read_unlock();
+}
+
+/* Invoked when a process calls exec() */
+void pmcs_exec_thread(struct task_struct *tsk)
+{
+	if(!sysctl_athena_enable)
+		return;
+
+	if (!pmc_callbacks.pmcs_exec_thread)
+		return;
+
+	rcu_read_lock();
+
+	pmc_callbacks.pmcs_exec_thread(tsk);
+
+	rcu_read_unlock();
+}
+
+/* Invoked when the kernel frees up the process descriptor */
+void pmcs_free_per_thread_data(struct task_struct *tsk)
+{
+	unsigned long flags;
+
+	if(!sysctl_athena_enable)
+		return;
+
+	read_lock_irqsave(&pmc_ops_lock, flags);
+
+	if (!pmc_callbacks.pmcs_free_per_thread_data) {
+		read_unlock_irqrestore(&pmc_ops_lock, flags);
+		return;
+	}
+
+	read_unlock_irqrestore(&pmc_ops_lock, flags);
+
+	pmc_callbacks.pmcs_free_per_thread_data(tsk);
+}
+
+/* Invoked when a process exits */
+void pmcs_exit_thread(struct task_struct *tsk)
+{
+	unsigned long flags;
+
+	if(!sysctl_athena_enable)
+		return;
+
+	read_lock_irqsave(&pmc_ops_lock, flags);
+
+	if (!pmc_callbacks.pmcs_exit_thread) {
+		read_unlock_irqrestore(&pmc_ops_lock, flags);
+		return;
+	}
+
+	read_unlock_irqrestore(&pmc_ops_lock, flags);
+
+	pmc_callbacks.pmcs_exit_thread(tsk);
+}
+
+/* Call this function to get value of a counter */
+int get_pmc_val(struct task_struct *task, uint64_t pmc_event, uint64_t *value)
+{
+	int ret = -1;
+
+	if(!sysctl_athena_enable)
+		return ret;
+
+	if (!pmc_callbacks.get_pmc_val)
+		return ret;
+
+	rcu_read_lock();
+
+	ret = pmc_callbacks.get_pmc_val(task, pmc_event, value);
+
+	rcu_read_unlock();
+
+	return ret;
+}
diff --git a/kernel/exit.c b/kernel/exit.c
index c3c7ac560114..8acfbbd0010a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -62,6 +62,7 @@
 #include <linux/random.h>
 #include <linux/rcuwait.h>
 #include <linux/compat.h>
+#include <linux/athena.h>
 
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
@@ -874,6 +875,9 @@ void __noreturn do_exit(long code)
 	perf_event_exit_task(tsk);
 
 	sched_autogroup_exit_task(tsk);
+#ifdef CONFIG_SCHED_ATHENA
+	pmcs_exit_thread(tsk);
+#endif
 	cgroup_exit(tsk);
 
 	/*
diff --git a/kernel/fork.c b/kernel/fork.c
index 0a2124c0a5f5..59150329e3ae 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -101,6 +101,7 @@
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 #include <asm/tlbflush.h>
+#include <linux/athena.h>
 
 #include <trace/events/sched.h>
 
@@ -148,6 +149,45 @@ int nr_processes(void)
 	return total;
 }
 
+#ifdef CONFIG_SCHED_ATHENA
+
+extern int least_bw_nid(void);
+extern unsigned int sysctl_athena_enable;
+
+/** 
+ *Finds the node of the page table based on pgd_t location
+ * in mm_struct 
+ */
+// static __always_inline int pgd_to_nid(pgd_t pgd) {
+// 	return page_to_nid(pgd_page(pgd));
+// }
+
+// static int count_node_processes(int node)
+// {
+// 	int processes = 0;
+//     struct task_struct *p, *t;
+// 	for_each_process_thread(p, t){
+// 		if (cpu_to_node(task_cpu(t)) == node){
+// 			++processes;
+// 		}
+// 	}
+// 	return processes;
+// }
+// /**
+//  * returns the NUMA node which has least number of processes
+//  * assigned to it.
+//  */
+// inline int least_process_loaded_node(void){
+// 	int node, count = 0, ret_node; 
+//     for_each_node(node){
+// 		// tmp = count_node_processes(node);
+// 		// if (tmp > count){
+// 		// }
+//     }
+// }
+
+#endif 
+
 void __weak arch_release_task_struct(struct task_struct *tsk)
 {
 }
@@ -371,6 +411,9 @@ void put_task_stack(struct task_struct *tsk)
 
 void free_task(struct task_struct *tsk)
 {
+#ifdef CONFIG_SCHED_ATHENA	
+	pmcs_free_per_thread_data(tsk);	
+#endif	
 #ifndef CONFIG_THREAD_INFO_IN_TASK
 	/*
 	 * The task is finally done with both the stack and thread_info,
@@ -1765,7 +1808,7 @@ static __latent_entropy struct task_struct *copy_process(
 	p->sequential_io	= 0;
 	p->sequential_io_avg	= 0;
 #endif
-
+	
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	retval = sched_fork(clone_flags, p);
 	if (retval)
@@ -2115,7 +2158,19 @@ long _do_fork(unsigned long clone_flags,
 		init_completion(&vfork);
 		get_task_struct(p);
 	}
-
+#ifdef CONFIG_SCHED_ATHENA
+	/**
+	 * @brief Athena: TODO: These might be done erlier in `copy process`/mpol_dup
+	 * This is a temporary workaround to bypass numactl
+	 */
+	// if(sysctl_athena_enable){
+	// 	if(!(clone_flags & CLONE_VM)){
+	// 		nodemask_t node_mask = nodemask_of_node(least_bw_nid());
+	// 		mpol_rebind_mm(p->mm, &node_mask);
+	// 		sched_setaffinity(p->pid, cpumask_of_node(least_bw_nid()));
+	// 	}
+	// }	
+#endif
 	wake_up_new_task(p);
 
 	/* forking complete and child started to run, tell ptracer */
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index e3336d904f64..e85b785b70c1 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -244,6 +244,9 @@ int __irq_set_affinity(unsigned int irq, const struct cpumask *mask, bool force)
 	raw_spin_unlock_irqrestore(&desc->lock, flags);
 	return ret;
 }
+#ifdef CONFIG_SCHED_ATHENA
+EXPORT_SYMBOL_GPL(__irq_set_affinity);
+#endif
 
 int irq_set_affinity_hint(unsigned int irq, const struct cpumask *m)
 {
diff --git a/kernel/irq_work.c b/kernel/irq_work.c
index 6b7cdf17ccf8..5b663212a426 100644
--- a/kernel/irq_work.c
+++ b/kernel/irq_work.c
@@ -86,6 +86,8 @@ bool irq_work_queue_on(struct irq_work *work, int cpu)
 	return true;
 }
 
+EXPORT_SYMBOL_GPL(irq_work_queue_on);
+
 /* Enqueue the irq work @work on the current CPU */
 bool irq_work_queue(struct irq_work *work)
 {
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 211890edf37e..c7b5daa440c6 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -21,6 +21,15 @@
 
 DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 
+#ifdef CONFIG_SCHED_ATHENA
+
+extern void update_pmcs(void);
+extern int least_bw_nid(void);
+DECLARE_PER_CPU(struct pmc_sample_struct, pmc_sample);
+
+#endif
+
+
 #if defined(CONFIG_SCHED_DEBUG) && defined(HAVE_JUMP_LABEL)
 /*
  * Debugging: various feature bits
@@ -2201,6 +2210,7 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	}
 
 	if (clone_flags & CLONE_VM)
+		/* set the preferred nid of the parent to the child task (thread) */
 		p->numa_preferred_nid = current->numa_preferred_nid;
 	else
 		p->numa_preferred_nid = -1;
@@ -2345,6 +2355,13 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 	 */
 	p->state = TASK_NEW;
 
+#ifdef CONFIG_SCHED_ATHENA
+	p->prof_enabled = 0;    /* The profiler will be disabled by default */
+	p->pmc = NULL;
+	if (pmcs_alloc_per_thread_data(clone_flags,p))
+		return -ENOMEM;
+#endif
+
 	/*
 	 * Make sure we do not leak PI boosting priority to the child.
 	 */
@@ -2718,6 +2735,9 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 	vtime_task_switch(prev);
 	perf_event_task_sched_in(prev, current);
 	finish_task(prev);
+#ifdef CONFIG_SCHED_ATHENA
+	pmcs_restore_callback(current->pmc, smp_processor_id()); 		
+#endif	
 	finish_lock_switch(rq);
 	finish_arch_post_lock_switch();
 
@@ -2997,6 +3017,9 @@ void sched_exec(void)
 	unsigned long flags;
 	int dest_cpu;
 
+#ifdef CONFIG_SCHED_ATHENA
+	pmcs_exec_thread(p);
+#endif	
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
 	dest_cpu = p->sched_class->select_task_rq(p, task_cpu(p), SD_BALANCE_EXEC, 0);
 	if (dest_cpu == smp_processor_id())
@@ -3095,6 +3118,10 @@ void scheduler_tick(void)
 
 	sched_clock_tick();
 
+#ifdef CONFIG_SCHED_ATHENA
+	pmcs_tbs_tick(curr->pmc, cpu);
+#endif
+
 	rq_lock(rq, &rf);
 
 	update_rq_clock(rq);
@@ -3106,7 +3133,17 @@ void scheduler_tick(void)
 
 	perf_event_task_tick();
 
+
 #ifdef CONFIG_SMP
+#ifdef CONFIG_SCHED_ATHENA
+	// if (sysctl_athena_enable){
+	// 	if ((this_cpu_ptr(&pmc_sample)->next_sample == 0) 
+	// 			|| time_after_eq(jiffies, this_cpu_ptr(&pmc_sample)->next_sample)){
+	// 		update_pmcs();
+	// 	}
+	// }
+#endif
+
 	rq->idle_balance = idle_cpu(cpu);
 	trigger_load_balance(rq);
 #endif
@@ -3512,6 +3549,9 @@ static void __sched notrace __schedule(bool preempt)
 		 */
 		++*switch_count;
 
+#ifdef CONFIG_SCHED_ATHENA
+		pmcs_save_callback(prev->pmc, cpu);
+#endif		
 		trace_sched_switch(preempt, prev, next);
 
 		/* Also unlocks the rq: */
@@ -4066,11 +4106,19 @@ struct task_struct *idle_task(int cpu)
  *
  * The task of @pid, if found. %NULL otherwise.
  */
+#ifdef CONFIG_SCHED_ATHENA
+struct task_struct *find_process_by_pid(pid_t pid)
+#else
 static struct task_struct *find_process_by_pid(pid_t pid)
+#endif
 {
 	return pid ? find_task_by_vpid(pid) : current;
 }
 
+#ifdef CONFIG_SCHED_ATHENA
+EXPORT_SYMBOL_GPL(find_process_by_pid);  
+#endif
+ 
 /*
  * sched_setparam() passes in -1 for its policy, to let the functions
  * it calls know not to change it.
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index edf3a5e7e728..c10766bec682 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -25,6 +25,16 @@
 
 #include <trace/events/sched.h>
 
+#ifdef CONFIG_NUMA
+#ifdef CONFIG_SCHED_ATHENA
+
+/* Holds a calculated number indicating the bandwidth bussiness of each NUMA node */
+// uint64_t numa_nodes_memory_usage_rate[NR_NODES] = {0};
+
+#endif
+#endif
+
+
 /*
  * Targeted preemption latency for CPU-bound tasks:
  *
@@ -1040,6 +1050,9 @@ unsigned int sysctl_numa_balancing_scan_delay = 1000;
 /* enable/disable control for pgtable migration feature */
 unsigned int sysctl_numa_pgtable_migration = 0;
 
+/* enable/disable for Athena pgtable-aware scheduler */
+unsigned int sysctl_athena_enable = 0;
+
 struct numa_group {
 	atomic_t refcount;
 
@@ -10450,3 +10463,162 @@ __init void init_sched_fair_class(void)
 #endif /* SMP */
 
 }
+
+#ifdef CONFIG_NUMA
+#ifdef CONFIG_SCHED_ATHENA
+/**
+ * @brief All functions in this sections need to be optimized
+ * 		  (e.g. inline, __always_inline, etc)
+ */
+
+#include <asm/msr.h>
+
+/**
+ * @brief Construct a new define per cpu object
+ */
+DEFINE_PER_CPU(struct pmc_sample_struct, pmc_sample);
+
+EXPORT_PER_CPU_SYMBOL(pmc_sample);
+
+/* TODO: Following functions might be better to be in independent module */
+
+/**
+ * @brief Triggers a pmc event selector
+ * 
+ * @param event_sel 
+ * @param store_register a general purpose pmu reg
+ */
+void start_pmc_event(u64 event_sel, u64 store_register){
+	wrmsrl(store_register, event_sel);
+}
+
+EXPORT_SYMBOL(start_pmc_event);
+
+/**
+ * @brief Triggers a pmc event selector for specific cpu
+ * 
+ * @param event_sel 
+ * @param store_register a general purpose pmu reg
+ */
+void start_cpu_pmc_event(int cpu, u64 event_sel, u64 store_register){
+	wrmsrl_on_cpu(cpu, store_register, event_sel);
+}
+
+EXPORT_SYMBOL(start_cpu_pmc_event);
+
+/**
+ * @brief read a single MSR register
+ * 
+ * @param from_register 
+ * @return u64 
+ */
+u64 read_pmc_val(u64 from_register){
+	u64 val;
+	rdmsrl(from_register, val);
+	return val;
+}
+
+EXPORT_SYMBOL(read_pmc_val);
+
+/**
+ * @brief update a single counter
+ * 
+ * @param event_sel 
+ */
+void update_pmc(unsigned long long event_sel){
+	u64 val;
+	switch (event_sel) {
+	case PMC_LLC_MISSES:
+		/* TODO: May be optimized */
+		val = read_pmc_val(MSR_IA32_PERFCTR0);
+		this_cpu_ptr(&pmc_sample)->llc_miss.rate
+					= val - this_cpu_ptr(&pmc_sample)->llc_miss.val;
+		this_cpu_ptr(&pmc_sample)->llc_miss.val = val;
+		break;
+	default:
+		// panic?!
+		break;
+	}
+}
+
+EXPORT_SYMBOL(update_pmc);
+
+/**
+ * @brief update all pmc_sample_struct counters
+ */
+void update_pmcs(void){
+	update_pmc(PMC_LLC_MISSES);
+	/* Read dTLB Misses and other counters */
+	this_cpu_ptr(&pmc_sample)->next_sample = jiffies + msecs_to_jiffies(PMC_UPDATE_PERIOD_MS);
+}
+
+EXPORT_SYMBOL(update_pmcs);
+
+/**
+ * @brief Returns the node id with least memory BW usage.
+ * TODO: This should be optimized to reduce the overhead of loops.
+ * 		 Probably with some data structure instead of dynamic calc.
+ */
+int least_bw_nid(void){
+	int node, cpu;
+	u64 ret_node = 0, node_rate = 0;
+	for_each_node(node){
+		u64 tmp = 0;
+		for_each_cpu(cpu, cpumask_of_node(node)){
+			tmp += per_cpu(pmc_sample, cpu).llc_miss.rate;
+		}
+		if ((node == 0) || (node_rate > tmp)){
+			node_rate = tmp;
+			ret_node = node;
+		}
+	}
+	return ret_node;
+}
+
+EXPORT_SYMBOL(least_bw_nid);
+
+void __athena_init_pmc(void){
+	// enable fixed function counters
+    wrmsrl(MSR_CORE_PERF_FIXED_CTR_CTRL, IA32_PERF_FIXED_CTRL_ENABLE);
+	// enable global general purpose counters
+    wrmsrl(MSR_CORE_PERF_GLOBAL_CTRL, IA32_PERF_GLOBAL_CTRL_ENABLE);
+}
+
+EXPORT_SYMBOL(__athena_init_pmc);
+
+/* enable/disable handler of sysctl_sched_enable variable.*/
+int sched_athena_enable_handler(struct ctl_table *table, int write, void __user *buffer,
+                                    size_t *lenp, loff_t *ppos)
+{
+	struct ctl_table t;
+	int err;
+
+	if (write && !capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	t = *table;
+	err = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);
+	if (err < 0)
+		return err;
+
+	/*
+	 * Perform required actions here.
+	 */
+	if(sysctl_athena_enable){
+		int cpu;
+		__athena_init_pmc();
+		// Trigger LLC miss counter
+		for_each_possible_cpu(cpu){
+			start_cpu_pmc_event(cpu, PMC_LLC_MISSES, MSR_P6_EVNTSEL0);
+		}
+	}else {
+		// TODO: reset pmu counters/registers
+	}
+	
+	printk(KERN_DEBUG "[ATHENA] enabled: %i\n", sysctl_athena_enable);
+	
+	return err;
+}
+
+#endif /* CONFIG_SCHED_ATHENA */
+#endif /* CONFIG_NUMA */
\ No newline at end of file
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index cb467c221b15..3f854fb55c82 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -51,6 +51,7 @@
 #include <linux/migrate.h>
 #include <linux/mmu_context.h>
 #include <linux/nmi.h>
+#include <linux/athena.h>
 #include <linux/proc_fs.h>
 #include <linux/prefetch.h>
 #include <linux/profile.h>
@@ -1038,6 +1039,7 @@ static inline void rq_repin_lock(struct rq *rq, struct rq_flags *rf)
 }
 
 #ifdef CONFIG_NUMA
+
 enum numa_topology_type {
 	NUMA_DIRECT,
 	NUMA_GLUELESS_MESH,
@@ -1046,6 +1048,7 @@ enum numa_topology_type {
 extern enum numa_topology_type sched_numa_topology_type;
 extern int sched_max_numa_distance;
 extern bool find_numa_distance(int distance);
+
 #endif
 
 #ifdef CONFIG_NUMA
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 01716f289ed2..3c22eb83148b 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -391,6 +391,15 @@ static struct ctl_table kern_table[] = {
 #endif /* CONFIG_SCHEDSTATS */
 #endif /* CONFIG_SMP */
 #ifdef CONFIG_NUMA
+#ifdef CONFIG_SCHED_ATHENA	
+	{
+		.procname   = "athena_enable",
+		.data		= &sysctl_athena_enable,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= sched_athena_enable_handler,
+	},
+#endif 	
 #ifdef CONFIG_PGTABLE_REPLICATION
 	{
 		.procname	= "pgtable_replication",
