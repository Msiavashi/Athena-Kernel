diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig
index a4d5f250b463..5372a9f5c0fc 100644
--- a/arch/x86/Kconfig
+++ b/arch/x86/Kconfig
@@ -2879,6 +2879,15 @@ config X86_SYSFB
 
 endmenu
 
+menu "Athena scheduler"
+
+config SCHED_ATHENA
+	bool "Enable support for Athena scheduler"
+	depends on NUMA
+	---help---
+		Include support for page-table aware cross-numa scheduling.
+		Please note that enabling this will disable HW counters for perf tool.
+endmenu
 
 menu "Executable file formats / Emulations"
 
@@ -2925,6 +2934,7 @@ config COMPAT
 	def_bool y
 	depends on IA32_EMULATION || X86_X32
 
+
 if COMPAT
 config COMPAT_FOR_U64_ALIGNMENT
 	def_bool y
diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c
index 45b2b1c93d04..2b8a764c7d25 100644
--- a/arch/x86/events/core.c
+++ b/arch/x86/events/core.c
@@ -1762,6 +1762,18 @@ ssize_t x86_event_sysfs_show(char *page, u64 config, u64 event)
 static struct attribute_group x86_pmu_attr_group;
 static struct attribute_group x86_pmu_caps_group;
 
+#ifdef CONFIG_SCHED_ATHENA
+/**
+ * @brief  This may cause problem for perf to profile applications.
+ * @todo Athena: fix this.
+ * @return int 
+ */
+static int __init init_hw_perf_events(void)
+{
+        printk(KERN_DEBUG "[Athena] is enabled: only software events are available in perf.\n");
+        return 0;
+}
+#else
 static int __init init_hw_perf_events(void)
 {
 	struct x86_pmu_quirk *quirk;
@@ -1883,6 +1895,7 @@ static int __init init_hw_perf_events(void)
 	cpuhp_remove_state(CPUHP_PERF_X86_PREPARE);
 	return err;
 }
+#endif
 early_initcall(init_hw_perf_events);
 
 static inline void x86_pmu_read(struct perf_event *event)
diff --git a/include/linux/athena.h b/include/linux/athena.h
new file mode 100644
index 000000000000..90f749126f37
--- /dev/null
+++ b/include/linux/athena.h
@@ -0,0 +1,27 @@
+#ifndef ATHENA_H
+#define ATHENA_H
+#include <linux/types.h>
+#include <linux/sched.h>
+
+struct athena_pmc_callbacks {
+	int (*pmcs_alloc_per_thread_data)(unsigned long, struct task_struct *);
+	void (*pmcs_save_callback)(struct task_struct *, int);
+	void (*pmcs_restore_callback)(struct task_struct *, int);
+	void (*pmcs_tbs_tick)(struct task_struct *, int);
+	void (*pmcs_exec_thread)(struct task_struct *);
+	void (*pmcs_free_per_thread_data)(struct task_struct *);
+	void (*pmcs_exit_thread)(struct task_struct *);
+	int (*get_pmc_val)(struct task_struct *task, uint64_t pmc_event,
+			   uint64_t *value);
+};
+
+int get_pmc_val(struct task_struct *task, uint64_t pmc_event, uint64_t *value);
+int pmcs_alloc_per_thread_data(unsigned long clone_flags,
+			       struct task_struct *p);
+void pmcs_save_callback(struct task_struct *task, int cpu);
+void pmcs_restore_callback(struct task_struct *task, int cpu);
+void pmcs_tbs_tick(struct task_struct *task, int cpu);
+void pmcs_exec_thread(struct task_struct *task);
+void pmcs_free_per_thread_data(struct task_struct *task);
+void pmcs_exit_thread(struct task_struct *task);
+#endif
\ No newline at end of file
diff --git a/include/linux/numa.h b/include/linux/numa.h
index 110b0e5d0fb0..33c3eca23dcb 100644
--- a/include/linux/numa.h
+++ b/include/linux/numa.h
@@ -13,4 +13,17 @@
 
 #define	NUMA_NO_NODE	(-1)
 
+#ifdef CONFIG_NUMA
+#ifdef CONFIG_SCHED_ATHENA
+
+/**
+ * TODO: This should be dynamically get number of sockets
+ * Currently this value only works for 2 socket machine.
+ * For more socket machines, this should be increased.
+ */
+#define NR_NODES 2
+
+#endif /* CONFIG_SCHED_ATHENA */
+#endif
+
 #endif /* _LINUX_NUMA_H */
diff --git a/include/linux/sched.h b/include/linux/sched.h
index ca3f3eae8980..95d7874c3da1 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -66,47 +66,49 @@ struct task_group;
  */
 
 /* Used in tsk->state: */
-#define TASK_RUNNING			0x0000
-#define TASK_INTERRUPTIBLE		0x0001
-#define TASK_UNINTERRUPTIBLE		0x0002
-#define __TASK_STOPPED			0x0004
-#define __TASK_TRACED			0x0008
+#define TASK_RUNNING 0x0000
+#define TASK_INTERRUPTIBLE 0x0001
+#define TASK_UNINTERRUPTIBLE 0x0002
+#define __TASK_STOPPED 0x0004
+#define __TASK_TRACED 0x0008
 /* Used in tsk->exit_state: */
-#define EXIT_DEAD			0x0010
-#define EXIT_ZOMBIE			0x0020
-#define EXIT_TRACE			(EXIT_ZOMBIE | EXIT_DEAD)
+#define EXIT_DEAD 0x0010
+#define EXIT_ZOMBIE 0x0020
+#define EXIT_TRACE (EXIT_ZOMBIE | EXIT_DEAD)
 /* Used in tsk->state again: */
-#define TASK_PARKED			0x0040
-#define TASK_DEAD			0x0080
-#define TASK_WAKEKILL			0x0100
-#define TASK_WAKING			0x0200
-#define TASK_NOLOAD			0x0400
-#define TASK_NEW			0x0800
-#define TASK_STATE_MAX			0x1000
+#define TASK_PARKED 0x0040
+#define TASK_DEAD 0x0080
+#define TASK_WAKEKILL 0x0100
+#define TASK_WAKING 0x0200
+#define TASK_NOLOAD 0x0400
+#define TASK_NEW 0x0800
+#define TASK_STATE_MAX 0x1000
 
 /* Convenience macros for the sake of set_current_state: */
-#define TASK_KILLABLE			(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
-#define TASK_STOPPED			(TASK_WAKEKILL | __TASK_STOPPED)
-#define TASK_TRACED			(TASK_WAKEKILL | __TASK_TRACED)
+#define TASK_KILLABLE (TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)
+#define TASK_STOPPED (TASK_WAKEKILL | __TASK_STOPPED)
+#define TASK_TRACED (TASK_WAKEKILL | __TASK_TRACED)
 
-#define TASK_IDLE			(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)
+#define TASK_IDLE (TASK_UNINTERRUPTIBLE | TASK_NOLOAD)
 
 /* Convenience macros for the sake of wake_up(): */
-#define TASK_NORMAL			(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)
+#define TASK_NORMAL (TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)
 
 /* get_task_state(): */
-#define TASK_REPORT			(TASK_RUNNING | TASK_INTERRUPTIBLE | \
-					 TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \
-					 __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE | \
-					 TASK_PARKED)
+#define TASK_REPORT                                                            \
+	(TASK_RUNNING | TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE |            \
+	 __TASK_STOPPED | __TASK_TRACED | EXIT_DEAD | EXIT_ZOMBIE |            \
+	 TASK_PARKED)
 
-#define task_is_traced(task)		((task->state & __TASK_TRACED) != 0)
+#define task_is_traced(task) ((task->state & __TASK_TRACED) != 0)
 
-#define task_is_stopped(task)		((task->state & __TASK_STOPPED) != 0)
+#define task_is_stopped(task) ((task->state & __TASK_STOPPED) != 0)
 
-#define task_is_stopped_or_traced(task)	((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
+#define task_is_stopped_or_traced(task)                                        \
+	((task->state & (__TASK_STOPPED | __TASK_TRACED)) != 0)
 
-#define task_contributes_to_load(task)	((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
+#define task_contributes_to_load(task)                                         \
+	((task->state & TASK_UNINTERRUPTIBLE) != 0 && \
 					 (task->flags & PF_FROZEN) == 0 && \
 					 (task->state & TASK_NOLOAD) == 0)
 
@@ -1168,6 +1170,10 @@ struct task_struct {
 	void				*security;
 #endif
 
+#ifdef CONFIG_SCHED_ATHENA 
+       struct pmc_sample_struct *pmc;                      /* Per-thread PMC-specific data. Would be allocated by the kernel module */
+       unsigned char prof_enabled;     /* This field must be one for the profiler to be active in the current task */
+#endif
 	/*
 	 * New fields for task_struct should be added above here, so that
 	 * they are included in the randomized portion of task_struct.
@@ -1764,4 +1770,23 @@ extern long sched_getaffinity(pid_t pid, struct cpumask *mask);
 #define TASK_SIZE_OF(tsk)	TASK_SIZE
 #endif
 
+#ifdef CONFIG_SCHED_ATHENA
+
+// #define PMC_UPDATE_PERIOD_MS 3000 /* The time interval to update the PMU counters */
+
+typedef struct {
+	u64 val;
+	u64 rate;
+	uint64_t rate_tsc;  /* cycles. not used yet */
+	uint64_t total_tsc;  /* cycles. not used yet */
+} pmc_t;
+
+struct pmc_sample_struct {
+	unsigned long next_sample_jiffies;  /* next jiffies to update the global sampling */
+	pmc_t llc_miss;
+	pmc_t dtlb_miss; /* not used yet */
+};
+
+#endif /* CONFIG_SCHED_ATHENA */
+
 #endif
diff --git a/include/linux/sched/sysctl.h b/include/linux/sched/sysctl.h
index 715c97c2d1f6..e5b5169bf368 100644
--- a/include/linux/sched/sysctl.h
+++ b/include/linux/sched/sysctl.h
@@ -7,6 +7,10 @@
 
 struct ctl_table;
 
+#ifdef CONFIG_SCHED_ATHENA
+extern unsigned int sysctl_athena_enable; /* 1 to enable */
+#endif
+
 #ifdef CONFIG_DETECT_HUNG_TASK
 extern int	     sysctl_hung_task_check_count;
 extern unsigned int  sysctl_hung_task_panic;
@@ -98,4 +102,8 @@ int sysctl_numa_pgtable_replication_misc_ctl(struct ctl_table *table, int write,
 			void __user *buffer, size_t *lenp, loff_t *ppos);
 int sysctl_numa_migrate_pid_pgtable_ctl(struct ctl_table *table, int write,
 			void __user *buffer, size_t *lenp, loff_t *ppos);
+
+// athena sysctl handler
+int sched_athena_enable_handler(struct ctl_table *table, int write,
+			       void __user *buffer, size_t *lenp, loff_t *ppos);
 #endif /* _LINUX_SCHED_SYSCTL_H */
diff --git a/kernel/Makefile b/kernel/Makefile
index f85ae5dfa474..45cb7c11c874 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -42,6 +42,7 @@ obj-y += irq/
 obj-y += rcu/
 obj-y += livepatch/
 
+obj-$(CONFIG_SCHED_ATHENA) += athena.o
 obj-$(CONFIG_CHECKPOINT_RESTORE) += kcmp.o
 obj-$(CONFIG_FREEZER) += freezer.o
 obj-$(CONFIG_PROFILING) += profile.o
diff --git a/kernel/athena.c b/kernel/athena.c
new file mode 100644
index 000000000000..35cfd6781926
--- /dev/null
+++ b/kernel/athena.c
@@ -0,0 +1,161 @@
+#include <linux/athena.h>
+#include <linux/list.h>
+#include <linux/err.h>
+#include <linux/slab.h>
+#include <linux/rwlock.h>
+#include <linux/rcupdate.h>
+
+extern unsigned int sysctl_athena_enable;
+DEFINE_RWLOCK(pmc_ops_lock);
+
+struct athena_pmc_callbacks pmc_callbacks;
+
+EXPORT_SYMBOL(pmc_callbacks);
+
+/* Invoked when forking a process/thread */
+int pmcs_alloc_per_thread_data(unsigned long clone_flags, struct task_struct *p)
+{
+	int ret = 0;
+	unsigned long flags;
+
+	if (!sysctl_athena_enable)
+		return 0;
+
+	read_lock_irqsave(&pmc_ops_lock, flags);
+
+	if (!pmc_callbacks.pmcs_alloc_per_thread_data) {
+		read_unlock_irqrestore(&pmc_ops_lock, flags);
+		return 0;
+	}
+
+	read_unlock_irqrestore(&pmc_ops_lock, flags);
+
+	/* Invoke the allocation operation (may block) */
+	ret = pmc_callbacks.pmcs_alloc_per_thread_data(clone_flags, p);
+
+	return ret;
+}
+
+/* Invoked when a context switch out takes place */
+void pmcs_save_callback(struct task_struct *task, int cpu)
+{
+	if (!sysctl_athena_enable)
+		return;
+
+	if (!pmc_callbacks.pmcs_save_callback)
+		return;
+
+	rcu_read_lock();
+
+	pmc_callbacks.pmcs_save_callback(task, cpu);
+
+	rcu_read_unlock();
+}
+
+/* Invoked when a context switch in takes place */
+void pmcs_restore_callback(struct task_struct *task, int cpu)
+{
+	if (!sysctl_athena_enable)
+		return;
+
+	if (!pmc_callbacks.pmcs_restore_callback)
+		return;
+
+	rcu_read_lock();
+
+	pmc_callbacks.pmcs_restore_callback(task, cpu);
+
+	rcu_read_unlock();
+}
+
+/* Invoked from scheduler_tick() */
+void pmcs_tbs_tick(struct task_struct *task, int cpu)
+{
+	if (!sysctl_athena_enable)
+		return;
+
+	if (!pmc_callbacks.pmcs_tbs_tick)
+		return;
+
+	rcu_read_lock();
+
+	pmc_callbacks.pmcs_tbs_tick(task, cpu);
+
+	rcu_read_unlock();
+}
+
+/* Invoked when a process calls exec() */
+void pmcs_exec_thread(struct task_struct *task)
+{
+	if (!sysctl_athena_enable)
+		return;
+
+	if (!pmc_callbacks.pmcs_exec_thread)
+		return;
+
+	rcu_read_lock();
+
+	pmc_callbacks.pmcs_exec_thread(task);
+
+	rcu_read_unlock();
+}
+
+/* Invoked when the kernel frees up the process descriptor */
+void pmcs_free_per_thread_data(struct task_struct *task)
+{
+	unsigned long flags;
+
+	if (!sysctl_athena_enable)
+		return;
+
+	read_lock_irqsave(&pmc_ops_lock, flags);
+
+	if (!pmc_callbacks.pmcs_free_per_thread_data) {
+		read_unlock_irqrestore(&pmc_ops_lock, flags);
+		return;
+	}
+
+	read_unlock_irqrestore(&pmc_ops_lock, flags);
+
+	pmc_callbacks.pmcs_free_per_thread_data(task);
+}
+
+/* Invoked when a process exits */
+void pmcs_exit_thread(struct task_struct *task)
+{
+	unsigned long flags;
+
+	if (!sysctl_athena_enable)
+		return;
+
+	read_lock_irqsave(&pmc_ops_lock, flags);
+
+	if (!pmc_callbacks.pmcs_exit_thread) {
+		read_unlock_irqrestore(&pmc_ops_lock, flags);
+		return;
+	}
+
+	read_unlock_irqrestore(&pmc_ops_lock, flags);
+
+	pmc_callbacks.pmcs_exit_thread(task);
+}
+
+/* Call this function to get value of a counter */
+int get_pmc_val(struct task_struct *task, uint64_t pmc_event, uint64_t *value)
+{
+	int ret = -1;
+
+	if (!sysctl_athena_enable)
+		return ret;
+
+	if (!pmc_callbacks.get_pmc_val)
+		return ret;
+
+	rcu_read_lock();
+
+	ret = pmc_callbacks.get_pmc_val(task, pmc_event, value);
+
+	rcu_read_unlock();
+
+	return ret;
+}
diff --git a/kernel/exit.c b/kernel/exit.c
index c3c7ac560114..8acfbbd0010a 100644
--- a/kernel/exit.c
+++ b/kernel/exit.c
@@ -62,6 +62,7 @@
 #include <linux/random.h>
 #include <linux/rcuwait.h>
 #include <linux/compat.h>
+#include <linux/athena.h>
 
 #include <linux/uaccess.h>
 #include <asm/unistd.h>
@@ -874,6 +875,9 @@ void __noreturn do_exit(long code)
 	perf_event_exit_task(tsk);
 
 	sched_autogroup_exit_task(tsk);
+#ifdef CONFIG_SCHED_ATHENA
+	pmcs_exit_thread(tsk);
+#endif
 	cgroup_exit(tsk);
 
 	/*
diff --git a/kernel/fork.c b/kernel/fork.c
index 0a2124c0a5f5..59150329e3ae 100644
--- a/kernel/fork.c
+++ b/kernel/fork.c
@@ -101,6 +101,7 @@
 #include <asm/mmu_context.h>
 #include <asm/cacheflush.h>
 #include <asm/tlbflush.h>
+#include <linux/athena.h>
 
 #include <trace/events/sched.h>
 
@@ -148,6 +149,45 @@ int nr_processes(void)
 	return total;
 }
 
+#ifdef CONFIG_SCHED_ATHENA
+
+extern int least_bw_nid(void);
+extern unsigned int sysctl_athena_enable;
+
+/** 
+ *Finds the node of the page table based on pgd_t location
+ * in mm_struct 
+ */
+// static __always_inline int pgd_to_nid(pgd_t pgd) {
+// 	return page_to_nid(pgd_page(pgd));
+// }
+
+// static int count_node_processes(int node)
+// {
+// 	int processes = 0;
+//     struct task_struct *p, *t;
+// 	for_each_process_thread(p, t){
+// 		if (cpu_to_node(task_cpu(t)) == node){
+// 			++processes;
+// 		}
+// 	}
+// 	return processes;
+// }
+// /**
+//  * returns the NUMA node which has least number of processes
+//  * assigned to it.
+//  */
+// inline int least_process_loaded_node(void){
+// 	int node, count = 0, ret_node; 
+//     for_each_node(node){
+// 		// tmp = count_node_processes(node);
+// 		// if (tmp > count){
+// 		// }
+//     }
+// }
+
+#endif 
+
 void __weak arch_release_task_struct(struct task_struct *tsk)
 {
 }
@@ -371,6 +411,9 @@ void put_task_stack(struct task_struct *tsk)
 
 void free_task(struct task_struct *tsk)
 {
+#ifdef CONFIG_SCHED_ATHENA	
+	pmcs_free_per_thread_data(tsk);	
+#endif	
 #ifndef CONFIG_THREAD_INFO_IN_TASK
 	/*
 	 * The task is finally done with both the stack and thread_info,
@@ -1765,7 +1808,7 @@ static __latent_entropy struct task_struct *copy_process(
 	p->sequential_io	= 0;
 	p->sequential_io_avg	= 0;
 #endif
-
+	
 	/* Perform scheduler related setup. Assign this task to a CPU. */
 	retval = sched_fork(clone_flags, p);
 	if (retval)
@@ -2115,7 +2158,19 @@ long _do_fork(unsigned long clone_flags,
 		init_completion(&vfork);
 		get_task_struct(p);
 	}
-
+#ifdef CONFIG_SCHED_ATHENA
+	/**
+	 * @brief Athena: TODO: These might be done erlier in `copy process`/mpol_dup
+	 * This is a temporary workaround to bypass numactl
+	 */
+	// if(sysctl_athena_enable){
+	// 	if(!(clone_flags & CLONE_VM)){
+	// 		nodemask_t node_mask = nodemask_of_node(least_bw_nid());
+	// 		mpol_rebind_mm(p->mm, &node_mask);
+	// 		sched_setaffinity(p->pid, cpumask_of_node(least_bw_nid()));
+	// 	}
+	// }	
+#endif
 	wake_up_new_task(p);
 
 	/* forking complete and child started to run, tell ptracer */
diff --git a/kernel/irq/manage.c b/kernel/irq/manage.c
index e3336d904f64..e85b785b70c1 100644
--- a/kernel/irq/manage.c
+++ b/kernel/irq/manage.c
@@ -244,6 +244,9 @@ int __irq_set_affinity(unsigned int irq, const struct cpumask *mask, bool force)
 	raw_spin_unlock_irqrestore(&desc->lock, flags);
 	return ret;
 }
+#ifdef CONFIG_SCHED_ATHENA
+EXPORT_SYMBOL_GPL(__irq_set_affinity);
+#endif
 
 int irq_set_affinity_hint(unsigned int irq, const struct cpumask *m)
 {
diff --git a/kernel/irq_work.c b/kernel/irq_work.c
index 6b7cdf17ccf8..5b663212a426 100644
--- a/kernel/irq_work.c
+++ b/kernel/irq_work.c
@@ -86,6 +86,8 @@ bool irq_work_queue_on(struct irq_work *work, int cpu)
 	return true;
 }
 
+EXPORT_SYMBOL_GPL(irq_work_queue_on);
+
 /* Enqueue the irq work @work on the current CPU */
 bool irq_work_queue(struct irq_work *work)
 {
diff --git a/kernel/sched/core.c b/kernel/sched/core.c
index 211890edf37e..001f928613e6 100644
--- a/kernel/sched/core.c
+++ b/kernel/sched/core.c
@@ -21,6 +21,15 @@
 
 DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 
+#ifdef CONFIG_SCHED_ATHENA
+
+extern void update_pmcs(void);
+extern int least_bw_nid(void);
+DECLARE_PER_CPU(struct pmc_sample_struct, pmc_sample);
+
+#endif
+
+
 #if defined(CONFIG_SCHED_DEBUG) && defined(HAVE_JUMP_LABEL)
 /*
  * Debugging: various feature bits
@@ -2201,6 +2210,7 @@ static void __sched_fork(unsigned long clone_flags, struct task_struct *p)
 	}
 
 	if (clone_flags & CLONE_VM)
+		/* set the preferred nid of the parent to the child task (thread) */
 		p->numa_preferred_nid = current->numa_preferred_nid;
 	else
 		p->numa_preferred_nid = -1;
@@ -2345,6 +2355,13 @@ int sched_fork(unsigned long clone_flags, struct task_struct *p)
 	 */
 	p->state = TASK_NEW;
 
+#ifdef CONFIG_SCHED_ATHENA
+	p->prof_enabled = 0;    /* The profiler will be disabled by default */
+	p->pmc = NULL;
+	if (pmcs_alloc_per_thread_data(clone_flags,p))
+		return -ENOMEM;
+#endif
+
 	/*
 	 * Make sure we do not leak PI boosting priority to the child.
 	 */
@@ -2718,6 +2735,9 @@ static struct rq *finish_task_switch(struct task_struct *prev)
 	vtime_task_switch(prev);
 	perf_event_task_sched_in(prev, current);
 	finish_task(prev);
+#ifdef CONFIG_SCHED_ATHENA
+	pmcs_restore_callback(current, smp_processor_id()); 		
+#endif	
 	finish_lock_switch(rq);
 	finish_arch_post_lock_switch();
 
@@ -2997,6 +3017,9 @@ void sched_exec(void)
 	unsigned long flags;
 	int dest_cpu;
 
+#ifdef CONFIG_SCHED_ATHENA
+	pmcs_exec_thread(p);
+#endif	
 	raw_spin_lock_irqsave(&p->pi_lock, flags);
 	dest_cpu = p->sched_class->select_task_rq(p, task_cpu(p), SD_BALANCE_EXEC, 0);
 	if (dest_cpu == smp_processor_id())
@@ -3095,6 +3118,10 @@ void scheduler_tick(void)
 
 	sched_clock_tick();
 
+#ifdef CONFIG_SCHED_ATHENA
+	pmcs_tbs_tick(curr, cpu);
+#endif
+
 	rq_lock(rq, &rf);
 
 	update_rq_clock(rq);
@@ -3106,7 +3133,17 @@ void scheduler_tick(void)
 
 	perf_event_task_tick();
 
+
 #ifdef CONFIG_SMP
+#ifdef CONFIG_SCHED_ATHENA
+	// if (sysctl_athena_enable){
+	// 	if ((this_cpu_ptr(&pmc_sample)->next_sample == 0) 
+	// 			|| time_after_eq(jiffies, this_cpu_ptr(&pmc_sample)->next_sample)){
+	// 		update_pmcs();
+	// 	}
+	// }
+#endif
+
 	rq->idle_balance = idle_cpu(cpu);
 	trigger_load_balance(rq);
 #endif
@@ -3512,6 +3549,9 @@ static void __sched notrace __schedule(bool preempt)
 		 */
 		++*switch_count;
 
+#ifdef CONFIG_SCHED_ATHENA
+		pmcs_save_callback(prev, cpu);
+#endif		
 		trace_sched_switch(preempt, prev, next);
 
 		/* Also unlocks the rq: */
@@ -4066,11 +4106,19 @@ struct task_struct *idle_task(int cpu)
  *
  * The task of @pid, if found. %NULL otherwise.
  */
+#ifdef CONFIG_SCHED_ATHENA
+struct task_struct *find_process_by_pid(pid_t pid)
+#else
 static struct task_struct *find_process_by_pid(pid_t pid)
+#endif
 {
 	return pid ? find_task_by_vpid(pid) : current;
 }
 
+#ifdef CONFIG_SCHED_ATHENA
+EXPORT_SYMBOL_GPL(find_process_by_pid);  
+#endif
+ 
 /*
  * sched_setparam() passes in -1 for its policy, to let the functions
  * it calls know not to change it.
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index edf3a5e7e728..934076b281f8 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -1040,6 +1040,11 @@ unsigned int sysctl_numa_balancing_scan_delay = 1000;
 /* enable/disable control for pgtable migration feature */
 unsigned int sysctl_numa_pgtable_migration = 0;
 
+/* enable/disable for Athena pgtable-aware scheduler */
+unsigned int sysctl_athena_enable = 0;
+
+EXPORT_SYMBOL(sysctl_athena_enable);
+
 struct numa_group {
 	atomic_t refcount;
 
@@ -10450,3 +10455,143 @@ __init void init_sched_fair_class(void)
 #endif /* SMP */
 
 }
+
+#ifdef CONFIG_NUMA
+#ifdef CONFIG_SCHED_ATHENA
+/**
+ * @brief All functions in this sections need to be optimized
+ * 		  (e.g. inline, __always_inline, etc)
+ */
+
+#include <asm/msr.h>
+
+/**
+ * @brief Construct a new define per cpu object
+ */
+// DEFINE_PER_CPU(struct pmc_sample_struct, pmc_sample);
+
+// EXPORT_PER_CPU_SYMBOL(pmc_sample);
+
+/* TODO: Following functions might be better to be in independent module */
+
+// /**
+//  * @brief Triggers a pmc event selector
+//  * 
+//  * @param event_sel 
+//  * @param store_register a general purpose pmu reg
+//  */
+// void start_pmc_event(u64 event_sel, u64 store_register){
+// 	wrmsrl(store_register, event_sel);
+// }
+
+// EXPORT_SYMBOL(start_pmc_event);
+
+/**
+ * @brief Triggers a pmc event selector for specific cpu
+ * 
+ * @param event_sel 
+ * @param store_register a general purpose pmu reg
+ */
+// void start_cpu_pmc_event(int cpu, u64 event_sel, u64 store_register){
+// 	wrmsrl_on_cpu(cpu, store_register, event_sel);
+// }
+
+// EXPORT_SYMBOL(start_cpu_pmc_event);
+
+// /**
+//  * @brief read a single MSR register
+//  * 
+//  * @param from_register 
+//  * @return u64 
+//  */
+// u64 read_pmc_val(u64 from_register){
+// 	u64 val;
+// 	rdmsrl(from_register, val);
+// 	return val;
+// }
+
+// EXPORT_SYMBOL(read_pmc_val);
+
+// /**
+//  * @brief update a single counter
+//  * 
+//  * @param event_sel 
+//  */
+// void update_pmc(unsigned long long event_sel){
+// 	u64 val;
+// 	switch (event_sel) {
+// 	case PMC_LLC_MISSES:
+// 		/* TODO: May be optimized */
+// 		val = read_pmc_val(MSR_IA32_PERFCTR0);
+// 		this_cpu_ptr(&pmc_sample)->llc_miss.rate
+// 					= val - this_cpu_ptr(&pmc_sample)->llc_miss.val;
+// 		this_cpu_ptr(&pmc_sample)->llc_miss.val = val;
+// 		break;
+// 	default:
+// 		// panic?!
+// 		break;
+// 	}
+// }
+
+// EXPORT_SYMBOL(update_pmc);
+
+// /**
+//  * @brief update all pmc_sample_struct counters
+//  */
+// void update_pmcs(void){
+// 	update_pmc(PMC_LLC_MISSES);
+// 	/* Read dTLB Misses and other counters */
+// 	this_cpu_ptr(&pmc_sample)->next_sample = jiffies + msecs_to_jiffies(PMC_UPDATE_PERIOD_MS);
+// }
+
+// EXPORT_SYMBOL(update_pmcs);
+
+// /**
+//  * @brief Returns the node id with least memory BW usage.
+//  * TODO: This should be optimized to reduce the overhead of loops.
+//  * 		 Probably with some data structure instead of dynamic calc.
+//  */
+// int least_bw_nid(void){
+// 	int node, cpu;
+// 	u64 ret_node = 0, node_rate = 0;
+// 	for_each_node(node){
+// 		u64 tmp = 0;
+// 		for_each_cpu(cpu, cpumask_of_node(node)){
+// 			tmp += per_cpu(pmc_sample, cpu).llc_miss.rate;
+// 		}
+// 		if ((node == 0) || (node_rate > tmp)){
+// 			node_rate = tmp;
+// 			ret_node = node;
+// 		}
+// 	}
+// 	return ret_node;
+// }
+
+// EXPORT_SYMBOL(least_bw_nid);
+
+/* enable/disable handler of sysctl_sched_enable variable.*/
+int sched_athena_enable_handler(struct ctl_table *table, int write, void __user *buffer,
+                                    size_t *lenp, loff_t *ppos)
+{
+	struct ctl_table t;
+	int err;
+
+	if (write && !capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	t = *table;
+	err = proc_dointvec_minmax(&t, write, buffer, lenp, ppos);
+	if (err < 0)
+		return err;
+
+	/*
+	 * Perform required actions here.
+	 */
+	
+	printk(KERN_DEBUG "[ATHENA] enabled: %i\n", sysctl_athena_enable);
+	
+	return err;
+}
+
+#endif /* CONFIG_SCHED_ATHENA */
+#endif /* CONFIG_NUMA */
\ No newline at end of file
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index cb467c221b15..3f854fb55c82 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -51,6 +51,7 @@
 #include <linux/migrate.h>
 #include <linux/mmu_context.h>
 #include <linux/nmi.h>
+#include <linux/athena.h>
 #include <linux/proc_fs.h>
 #include <linux/prefetch.h>
 #include <linux/profile.h>
@@ -1038,6 +1039,7 @@ static inline void rq_repin_lock(struct rq *rq, struct rq_flags *rf)
 }
 
 #ifdef CONFIG_NUMA
+
 enum numa_topology_type {
 	NUMA_DIRECT,
 	NUMA_GLUELESS_MESH,
@@ -1046,6 +1048,7 @@ enum numa_topology_type {
 extern enum numa_topology_type sched_numa_topology_type;
 extern int sched_max_numa_distance;
 extern bool find_numa_distance(int distance);
+
 #endif
 
 #ifdef CONFIG_NUMA
diff --git a/kernel/sysctl.c b/kernel/sysctl.c
index 01716f289ed2..3c22eb83148b 100644
--- a/kernel/sysctl.c
+++ b/kernel/sysctl.c
@@ -391,6 +391,15 @@ static struct ctl_table kern_table[] = {
 #endif /* CONFIG_SCHEDSTATS */
 #endif /* CONFIG_SMP */
 #ifdef CONFIG_NUMA
+#ifdef CONFIG_SCHED_ATHENA	
+	{
+		.procname   = "athena_enable",
+		.data		= &sysctl_athena_enable,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= sched_athena_enable_handler,
+	},
+#endif 	
 #ifdef CONFIG_PGTABLE_REPLICATION
 	{
 		.procname	= "pgtable_replication",
